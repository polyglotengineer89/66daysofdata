{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3be056d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./LossFunction/1.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "Image(url= \"./LossFunction/1.png\", width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ab79d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./LossFunction/2.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./LossFunction/2.png\", width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5856204a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./LossFunction/3.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./LossFunction/3.png\", width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29383c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./LossFunction/4.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./LossFunction/4.png\", width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4563b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./LossFunction/5.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./LossFunction/5.png\", width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d1af2",
   "metadata": {},
   "source": [
    "<h4>Exercise</h4>\n",
    "<h2>Changing the model coefficients</h2>\n",
    "When you call fit with scikit-learn, the logistic regression coefficients are automatically learned from your dataset. In this exercise you will explore how the decision boundary is represented by the coefficients. To do so, you will change the coefficients manually (instead of with fit), and visualize the resulting classifiers.\n",
    "\n",
    "A 2D dataset is already loaded into the environment as X and y, along with a linear classifier object model.\n",
    "\n",
    "<h4>Instructions</h4>\n",
    "<ul>\n",
    "    <li>Set the two coefficients and the intercept to various values and observe the resulting decision boundaries.</li>\n",
    "    <li>Try to build up a sense of how the coefficients relate to the decision boundary.</li>\n",
    "    <li>Set the coefficients and intercept such that the model makes no errors on the given training data.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2afc813",
   "metadata": {},
   "source": [
    "# Set the coefficients\n",
    "model.coef_ = np.array([[0,1]])\n",
    "model.intercept_ = np.array([0])\n",
    "\n",
    "# Plot the data and decision boundary\n",
    "plot_classifier(X,y,model)\n",
    "\n",
    "# Print the number of errors\n",
    "num_err = np.sum(y != model.predict(X))\n",
    "print(\"Number of errors:\", num_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60dd3d2",
   "metadata": {},
   "source": [
    "Great job! As you've been experiencing, the coefficients determine the slope of the boundary and the intercept shifts it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fcda475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./LossFunction/6.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./LossFunction/6.png\", width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f400c016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./LossFunction/7.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./LossFunction/7.png\", width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "617fca57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./LossFunction/8.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./LossFunction/8.png\", width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "511b7189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./LossFunction/9.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./LossFunction/9.png\", width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02e80fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./LossFunction/10.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./LossFunction/10.png\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e4f41",
   "metadata": {},
   "source": [
    "Answer: 2\n",
    "Correct! There is 1 misclassified red point and 1 misclassified blue point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e411cbb8",
   "metadata": {},
   "source": [
    "<h4>Exercise</h4>\n",
    "<h2>Minimizing a loss function</h2>\n",
    "In this exercise you'll implement linear regression \"from scratch\" using scipy.optimize.minimize.\n",
    "\n",
    "We'll train a model on the Boston housing price data set, which is already loaded into the variables X and y. For simplicity, we won't include an intercept in our regression model.\n",
    "\n",
    "<h4>Instructions</h4>\n",
    "<ul>\n",
    "    <li>Fill in the loss function for least squares linear regression.</li>\n",
    "    <li>Print out the coefficients from fitting sklearn's LinearRegression.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f648ae18",
   "metadata": {},
   "source": [
    "<h4>Exercise</h4>\n",
    "<h2>Minimizing a loss function</h2>\n",
    "In this exercise you'll implement linear regression \"from scratch\" using scipy.optimize.minimize.\n",
    "\n",
    "We'll train a model on the Boston housing price data set, which is already loaded into the variables X and y. For simplicity, we won't include an intercept in our regression model.\n",
    "\n",
    "<h4>Instructions</h4>\n",
    "<ul>\n",
    "    <li>Fill in the loss function for least squares linear regression.</li>\n",
    "    <li>Print out the coefficients from fitting sklearn's LinearRegression.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dbba5edc",
   "metadata": {},
   "source": [
    "# The squared error, summed over training examples\n",
    "def my_loss(w):\n",
    "    s = 0\n",
    "    for i in range(y.size):\n",
    "        # Get the true and predicted target values for example 'i'\n",
    "        y_i_true = y[i]\n",
    "        y_i_pred = w@X[i]\n",
    "        s = s + (y_i_pred - y_i_true)**2\n",
    "    return s\n",
    "\n",
    "# Returns the w that makes my_loss(w) smallest\n",
    "w_fit = minimize(my_loss, X[0]).x\n",
    "print(w_fit)\n",
    "\n",
    "# Compare with scikit-learn's LinearRegression coefficients\n",
    "lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2544b9f",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "    [-9.16299112e-02  4.86754828e-02 -3.77698794e-03  2.85635998e+00\n",
    "     -2.88057050e+00  5.92521269e+00 -7.22470732e-03 -9.67992974e-01\n",
    "      1.70448714e-01 -9.38971600e-03 -3.92421893e-01  1.49830571e-02\n",
    "     -4.16973012e-01]\n",
    "    [-9.16297843e-02  4.86751203e-02 -3.77930006e-03  2.85636751e+00\n",
    "     -2.88077933e+00  5.92521432e+00 -7.22447929e-03 -9.67995240e-01\n",
    "      1.70443393e-01 -9.38925373e-03 -3.92425680e-01  1.49832102e-02\n",
    "     -4.16972624e-01]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f9151",
   "metadata": {},
   "source": [
    "Great job! This was a tough one. Isn't it cool how you reproduce the weights learned by scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7df85467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./LossFunction/11.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./LossFunction/11.png\", width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0220ca3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./LossFunction/12.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./LossFunction/12.png\", width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06866c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./LossFunction/13.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./LossFunction/13.png\", width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b6f4ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./LossFunction/14.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./LossFunction/14.png\", width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "401edc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./LossFunction/15.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./LossFunction/15.png\", width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51cd5f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./LossFunction/16.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./LossFunction/16.png\", width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f16169eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./LossFunction/17.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./LossFunction/17.png\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076bf9cb",
   "metadata": {},
   "source": [
    "Answer: 2<br>\n",
    "Correct! This loss is very similar to the hinge loss used in SVMs (just shifted slightly)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e91eac",
   "metadata": {},
   "source": [
    "<h4>Exercise</h4>\n",
    "<h3>Comparing the logistic and hinge losses</h3>\n",
    "In this exercise you'll create a plot of the logistic and hinge losses using their mathematical expressions, which are provided to you.\n",
    "\n",
    "The loss function diagram from the video is shown on the right.\n",
    "\n",
    "<h4>Instructions</h4>\n",
    "Evaluate the log_loss() and hinge_loss() functions at the grid points so that they are plotted."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c69c78aa",
   "metadata": {},
   "source": [
    "# Mathematical functions for logistic and hinge losses\n",
    "def log_loss(raw_model_output):\n",
    "   return np.log(1+np.exp(-raw_model_output))\n",
    "   \n",
    "def hinge_loss(raw_model_output):\n",
    "   return np.maximum(0,1-raw_model_output)\n",
    "\n",
    "# Create a grid of values and plot\n",
    "grid = np.linspace(-2,2,1000)\n",
    "plt.plot(grid, log_loss(grid), label='logistic')\n",
    "plt.plot(grid, hinge_loss(grid), label='hinge')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3e6e93",
   "metadata": {},
   "source": [
    "Nice! As you can see, these match up with the loss function diagrams we saw in the video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aadeef",
   "metadata": {},
   "source": [
    "<h4>Exercise</h4>\n",
    "<h3>Implementing logistic regression</h3>\n",
    "This is very similar to the earlier exercise where you implemented linear regression \"from scratch\" using scipy.optimize.minimize. However, this time we'll minimize the logistic loss and compare with scikit-learn's LogisticRegression (we've set C to a large value to disable regularization; more on this in Chapter 3!).\n",
    "\n",
    "The log_loss() function from the previous exercise is already defined in your environment, and the sklearn breast cancer prediction dataset (first 10 features, standardized) is loaded into the variables X and y.\n",
    "\n",
    "<h3>Instructions</h3>\n",
    "<ul>\n",
    "    <li>Input the number of training examples into range().</li>\n",
    "    <li>Fill in the loss function for logistic regression.</li>\n",
    "    <li>Compare the coefficients to sklearn's LogisticRegression.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f697e9ca",
   "metadata": {},
   "source": [
    "# The logistic loss, summed over training examples\n",
    "def my_loss(w):\n",
    "    s = 0\n",
    "    for i in range(y.size):\n",
    "        raw_model_output = w@X[i]\n",
    "        s = s + log_loss(raw_model_output * y[i])\n",
    "    return s\n",
    "\n",
    "# Returns the w that makes my_loss(w) smallest\n",
    "w_fit = minimize(my_loss, X[0]).x\n",
    "print(w_fit)\n",
    "\n",
    "# Compare with scikit-learn's LogisticRegression\n",
    "lr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14b78c68",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "    [ 1.03592182 -1.65378492  4.08331342 -9.40923002 -1.06786489  0.07892114\n",
    "     -0.85110344 -2.44103305 -0.45285671  0.43353448]\n",
    "    [[ 1.03731085 -1.65339037  4.08143924 -9.40788356 -1.06757746  0.07895582\n",
    "      -0.85072003 -2.44079089 -0.45271     0.43334997]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92db896",
   "metadata": {},
   "source": [
    "Great job! As you can see, logistic regression is just minimizing the loss function we've been looking at. Much more on logistic regression in the next chapter!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
